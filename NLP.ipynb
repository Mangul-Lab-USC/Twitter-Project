{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Analysis : Qantitative Facts on the Twitter Usage of Scientists in Bioinformatics and Medicine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/louismockly/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/louismockly/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lets import the fundamental packages\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Lets write son function that are going to clean the sentences\n",
    "\n",
    "# This function is very important because it is going to remove the URL and tweets have a lot\n",
    "def remove_mentionUrls(text):\n",
    "    tweet_out = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    re.sub('https?://[A-Za-z0-9]+', '', tweet_out)\n",
    "    return tweet_out\n",
    "\n",
    "# This one removes all the punctuations\n",
    "def remove_nonalphanumeric(text):\n",
    "    text_out = ''.join([char for char in text if char not in string.punctuation])\n",
    "    return text_out\n",
    "\n",
    "# This funnction removes all the stopwords that are listed in the stopwords_list that we import juste bellow\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "# This function removes the grammatical part of all the verbs\n",
    "def stemming(input_text):\n",
    "    porter = PorterStemmer()\n",
    "    words = input_text.split() \n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "# This function removes the words that are not frequent\n",
    "def to_lower(input_text):\n",
    "    return input_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Twitter NLP Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n"
     ]
    }
   ],
   "source": [
    "# We first import 5000 positive and 5000 negative tweets from NLTK database\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "pos_tweets_split = [(text.split(), 'pos') for text in pos_tweets]\n",
    "neg_tweets_split = [(text.split(), 'neg') for text in neg_tweets]\n",
    "tot_tweets_split = pos_tweets_split + neg_tweets_split\n",
    "random.shuffle(tot_tweets_split)\n",
    "\n",
    "# We clean the data and change it into a good format\n",
    "clean = [to_lower(stemming(remove_stopwords(remove_nonalphanumeric(remove_mentionUrls(text)))))\n",
    "         for text in pos_tweets + neg_tweets]\n",
    "words = [text.split() for text in clean]\n",
    "words = [item for elem in words for item in elem]\n",
    "\n",
    "# We extract the 2000 most frequent words from the campus\n",
    "all_words = nltk.FreqDist(w.lower() for w in words)\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "# We create the features\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# NaiveBayse Classifier\n",
    "featuresets = [(document_features(d), c) for (d,c) in tot_tweets_split]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# We print the score\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sentiment Analysis with TFiDF and Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to try a second method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We import TfidfVectorizer and the Random Forest Classifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# We load the Data with tweets, tweets clean and poitive and negative classification\n",
    "df_data = pd.read_excel('/Users/louismockly/Documents/Twitter_Project/sentiment_analysis.xlsx')\n",
    "df_data = df_data.drop(['Unnamed: 0'], 1)\n",
    "\n",
    "# We only keep the data with a minimum len of letters to keep a sentences without noise\n",
    "df_data = df_data[df_data.tweets_clean.str.len() > 7]\n",
    "df_data = df_data.reset_index()\n",
    "df_data = df_data.drop(['index'], 1)\n",
    "\n",
    "# We apply the TFIDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = df_data.tweets_clean\n",
    "response = vectorizer.fit_transform(X)\n",
    "tab = response.toarray()\n",
    "df_tfidef = pd.DataFrame(tab)\n",
    "df_ML = pd.concat([df_data[['class']], df_tfidef], 1)\n",
    "X, y = df_ML.drop(['class'], 1).values, np.array(df_ML['class'])\n",
    "\n",
    "# We split the data into train and test \n",
    "X_train, X_test, y_train, y_test = X[500:], X[:500], y[500:], y[:500]\n",
    "\n",
    "# We apply a random forest \n",
    "clf = RandomForestClassifier(n_estimators=60)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# We print the score\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bert Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are goind to import the BERT model developped by google. The embedding is very powerfull but very low. Thus we are only goind to code the function that gives sentences embedding if you want to try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
      "662904832/662903077 [==============================] - 1331s 2us/step\n",
      "/Users/louismockly/.keras/datasets/multi_cased_L-12_H-768_A-12/bert_config.json /Users/louismockly/.keras/datasets/multi_cased_L-12_H-768_A-12/bert_model.ckpt /Users/louismockly/.keras/datasets/multi_cased_L-12_H-768_A-12/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# We import Bert Model\n",
    "from keras_bert import get_pretrained, PretrainedList, get_checkpoint_paths\n",
    "model_path = get_pretrained(PretrainedList.multi_cased_base)\n",
    "paths = get_checkpoint_paths(model_path)\n",
    "print(paths.config, paths.checkpoint, paths.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import extract_embeddings\n",
    "# We load the Data with tweets, tweets clean and poitive and negative classification\n",
    "df_data = pd.read_excel('/Users/louismockly/Documents/Twitter_Project/sentiment_analysis.xlsx')\n",
    "df_data = df_data.drop(['Unnamed: 0'], 1)\n",
    "\n",
    "# We only keep the data with a minimum len of letters\n",
    "df_data = df_data[df_data.tweets_clean.str.len() > 7]\n",
    "df_data = df_data.reset_index()\n",
    "df_data = df_data.drop(['index'], 1)\n",
    "\n",
    "# Here, it gives the embedding of the sentences you want\n",
    "model_path = '/Users/louismockly/.keras/datasets/multi_cased_L-12_H-768_A-12'\n",
    "texts = df_data.tweets_clean\n",
    "embeddings = extract_embeddings(model_path, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pro Perso with TFiDF and Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to try the TF-IDF and a Random Forest Classifier to the Pro-Perso data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We import the data \n",
    "df_pro_perso = pd.read_excel('/Users/louismockly/Documents/Twitter_Project/pro_perso.xlsx')\n",
    "df_pro_perso = df_pro_perso.drop(['Unnamed: 0'], 1)\n",
    "df_pro_perso.columns = ['tweet', 'pro_perso']\n",
    "df_pro_perso = df_pro_perso.dropna()\n",
    "df_pro_perso = df_pro_perso.reset_index()\n",
    "df_pro_perso = df_pro_perso.drop(['index'], 1)\n",
    "\n",
    "# We Clean the data\n",
    "tweet_pro_perso_clean = [to_lower(stemming(remove_stopwords(remove_nonalphanumeric(remove_mentionUrls(text))))) \\\n",
    "         for text in df_pro_perso.tweet]\n",
    "df_tweet_clean = pd.DataFrame(tweet_pro_perso_clean, columns = ['tweet_clean'])\n",
    "df_pro_perso_clean = pd.concat([df_tweet_clean, df_pro_perso], 1)\n",
    "df_pro_perso_clean = df_pro_perso_clean.replace('Pro', 1)\n",
    "df_pro_perso_clean = df_pro_perso_clean.replace('Perso', 0)\n",
    "\n",
    "# We vectorize it\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = df_pro_perso_clean.tweet_clean\n",
    "response = vectorizer.fit_transform(X)\n",
    "tab = response.toarray()\n",
    "df_tfidef = pd.DataFrame(tab)\n",
    "df_ML = pd.concat([df_pro_perso_clean[['pro_perso']], df_tfidef], 1)\n",
    "X, y = df_ML.drop(['pro_perso'], 1).values, np.array(df_ML['pro_perso'])\n",
    "\n",
    "# We split intot train and test set\n",
    "X_train, X_test, y_train, y_test = X[50:], X[:50], y[50:], y[:50]\n",
    "\n",
    "# Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CNN the Best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louismockly/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7893 samples, validate on 1393 samples\n",
      "Epoch 1/5\n",
      " - 51s - loss: 0.5156 - accuracy: 0.7322 - val_loss: 0.4600 - val_accuracy: 0.7861\n",
      "Epoch 2/5\n",
      " - 46s - loss: 0.2244 - accuracy: 0.9117 - val_loss: 0.4902 - val_accuracy: 0.7918\n",
      "Epoch 3/5\n",
      " - 44s - loss: 0.0427 - accuracy: 0.9867 - val_loss: 0.6879 - val_accuracy: 0.7782\n",
      "Epoch 4/5\n",
      " - 44s - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.7929 - val_accuracy: 0.7846\n",
      "Epoch 5/5\n",
      " - 45s - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.9253 - val_accuracy: 0.7803\n",
      "score: 0.93\n",
      "acc: 0.78\n"
     ]
    }
   ],
   "source": [
    "# We load the Data with tweets, tweets clean and poitive and negative classification\n",
    "df_data = pd.read_excel('/Users/louismockly/Documents/Twitter_Project/sentiment_analysis.xlsx')\n",
    "df_data = df_data.drop(['Unnamed: 0'], 1)\n",
    "\n",
    "# We only keep the data with a minimum len of letters\n",
    "df_data = df_data[df_data.tweets_clean.str.len() > 7]\n",
    "df_data = df_data.reset_index()\n",
    "df_data = df_data.drop(['index'], 1)\n",
    "\n",
    "df_data = df_data.drop(['tweets_clean'], 1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 2000\n",
    "\n",
    "# We split the data\n",
    "x_train, x_validation, y_train, y_validation = \\\n",
    "                train_test_split(df_data.tweets, df_data['class'], test_size=.15, random_state=SEED)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))\n",
    "max(length)\n",
    "\n",
    "x_train_seq = pad_sequences(sequences, maxlen=max(length) + 5)\n",
    "\n",
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=max(length) + 5)\n",
    "\n",
    "\n",
    "#CNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from time import time\n",
    "\n",
    "acc = []\n",
    "times = []\n",
    "\n",
    "model_cnn = Sequential()\n",
    "\n",
    "# Embedding and import the model\n",
    "e = Embedding(100000, 100, input_length=max(length) + 5)\n",
    "model_cnn.add(e)\n",
    "model_cnn.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(256, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# We print the score and accuracy of the model at each step\n",
    "t0 = time()\n",
    "model_cnn.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)\n",
    "score,accu = model_cnn.evaluate(x_val_seq, y_validation, verbose = 2, batch_size = 32)\n",
    "tv_time = time()-t0\n",
    "\n",
    "acc.append(accu*100)\n",
    "times.append(tv_time*0.0166667)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pro VS Perso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louismockly/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 433 samples, validate on 49 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 0.6707 - accuracy: 0.5912 - val_loss: 0.6738 - val_accuracy: 0.5510\n",
      "Epoch 2/5\n",
      " - 2s - loss: 0.6247 - accuracy: 0.5958 - val_loss: 0.6412 - val_accuracy: 0.5510\n",
      "Epoch 3/5\n",
      " - 2s - loss: 0.5267 - accuracy: 0.6859 - val_loss: 0.5572 - val_accuracy: 0.6735\n",
      "Epoch 4/5\n",
      " - 2s - loss: 0.3240 - accuracy: 0.9376 - val_loss: 0.4264 - val_accuracy: 0.8571\n",
      "Epoch 5/5\n",
      " - 2s - loss: 0.1116 - accuracy: 0.9954 - val_loss: 0.3802 - val_accuracy: 0.8980\n",
      "score: 0.38\n",
      "acc: 0.90\n"
     ]
    }
   ],
   "source": [
    "# We import the data\n",
    "df_pro_perso = pd.read_excel('/Users/louismockly/Documents/Twitter_Project/pro_perso.xlsx')\n",
    "df_pro_perso = df_pro_perso.drop(['Unnamed: 0'], 1)\n",
    "df_pro_perso.columns = ['tweet', 'pro_perso']\n",
    "df_pro_perso = df_pro_perso.dropna()\n",
    "df_pro_perso = df_pro_perso.reset_index()\n",
    "df_pro_perso = df_pro_perso.drop(['index'], 1)\n",
    "df_pro_perso = df_pro_perso_clean.replace('Pro', 1)\n",
    "df_pro_perso = df_pro_perso_clean.replace('Perso', 0)\n",
    "\n",
    "df_data = df_pro_perso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 2000\n",
    "\n",
    "# We split the data\n",
    "x_train, x_validation, y_train, y_validation = \\\n",
    "                train_test_split(df_data.tweet, df_data['pro_perso'], test_size=.10, random_state=SEED)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))\n",
    "max(length)\n",
    "\n",
    "x_train_seq = pad_sequences(sequences, maxlen=max(length) + 5)\n",
    "\n",
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=max(length) + 5)\n",
    "\n",
    "\n",
    "#CNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from time import time\n",
    "\n",
    "acc = []\n",
    "times = []\n",
    "\n",
    "model_cnn = Sequential()\n",
    "\n",
    "# Embedding and import the model\n",
    "e = Embedding(100000, 100, input_length=max(length) + 5)\n",
    "model_cnn.add(e)\n",
    "model_cnn.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(256, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# We print the score and accuracy of the model at each step\n",
    "t0 = time()\n",
    "model_cnn.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)\n",
    "score,accu = model_cnn.evaluate(x_val_seq, y_validation, verbose = 2, batch_size = 32)\n",
    "tv_time = time()-t0\n",
    "\n",
    "acc.append(accu*100)\n",
    "times.append(tv_time*0.0166667)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (accu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
